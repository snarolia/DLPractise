{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c5b1a1",
   "metadata": {},
   "source": [
    "Perfect — let’s nail **R² (R-squared)** with maximum clarity 🔥\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 What is R²?\n",
    "\n",
    "* R² (coefficient of determination) tells us **how well a regression model explains the variability of the data**.\n",
    "* It’s a **goodness-of-fit** measure.\n",
    "\n",
    "👉 Think of it as:\n",
    "**“Out of all the variation in `y`, how much of it does my model explain?”**\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Formula (but keep it intuitive)\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $SS_{res}$ = Residual Sum of Squares = variation the model **didn’t** explain.\n",
    "* $SS_{tot}$ = Total Sum of Squares = total variation in `y` (baseline).\n",
    "\n",
    "👉 So, $\\frac{SS_{res}}{SS_{tot}}$ = % of unexplained variance.\n",
    "👉 Then, $1 -$ that = % of variance explained by the model.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Minimal Example\n",
    "\n",
    "Suppose we have data:\n",
    "\n",
    "| Hours Studied (X) | Exam Score (Y) |\n",
    "| ----------------- | -------------- |\n",
    "| 1                 | 50             |\n",
    "| 2                 | 60             |\n",
    "| 3                 | 65             |\n",
    "| 4                 | 70             |\n",
    "| 5                 | 80             |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Compute Mean of Y\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{50+60+65+70+80}{5} = 65\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Total Variation (SS\\_tot)\n",
    "\n",
    "$$\n",
    "SS_{tot} = \\sum (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "\\= (50-65)² + (60-65)² + (65-65)² + (70-65)² + (80-65)²\n",
    "\\= 225 + 25 + 0 + 25 + 225\n",
    "\\= **500**\n",
    "\n",
    "👉 This is the **total spread of data around the mean**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Fit a Simple Regression\n",
    "\n",
    "Say our model predicts:\n",
    "\n",
    "| X | Predicted ŷ |\n",
    "| - | ----------- |\n",
    "| 1 | 52          |\n",
    "| 2 | 60          |\n",
    "| 3 | 66          |\n",
    "| 4 | 72          |\n",
    "| 5 | 80          |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Residual Variation (SS\\_res)\n",
    "\n",
    "$$\n",
    "SS_{res} = \\sum (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\\= (50-52)² + (60-60)² + (65-66)² + (70-72)² + (80-80)²\n",
    "\\= 4 + 0 + 1 + 4 + 0\n",
    "\\= **9**\n",
    "\n",
    "👉 Only 9 units of variation are unexplained!\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Compute R²\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{9}{500} = 0.982\n",
    "$$\n",
    "\n",
    "✅ **R² = 0.982 (98.2%)** → The model explains almost all the variation in exam scores.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Intuition Recap\n",
    "\n",
    "* If **R² = 0** → model explains nothing (no better than just using the mean).\n",
    "* If **R² = 1** → perfect fit (model explains all variance).\n",
    "* If **R² = 0.7** → model explains 70% of variance, 30% is still noise.\n",
    "* R² can even be **negative** if your model is worse than just predicting the mean.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Simple Analogy\n",
    "\n",
    "Imagine trying to predict **how much water is in a glass**:\n",
    "\n",
    "* **SS\\_tot** = total mess of guesses if you just use the “average amount of water” every time.\n",
    "* **SS\\_res** = leftover mess after using your smart model.\n",
    "* **R²** = how much better your model is at cleaning up the mess compared to just using the average.\n",
    "\n",
    "---\n",
    "\n",
    "✅ So R² is **not magic** — it’s just:\n",
    "**“How much better is my model than just using the average?”**\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn.metrics.r2_score`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee22dae",
   "metadata": {},
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    ">Mean Absolute Error: this is an interpretable metric because it has the same unit of measurment as the initial series,  [0,+∞)\n",
    " \n",
    ">MAE=∑i=1n|yi−ŷ i|n\n",
    "\n",
    "`sklearn.metrics.mean_absolute_error`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08788de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65e43ff6",
   "metadata": {},
   "source": [
    "# Median Absolute Error (MedAE)\n",
    "MedAE=median(|y1−ŷ 1|,...,|yn−ŷ n|)\n",
    "\n",
    "`sklearn.metrics.median_absolute_error`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2fabf",
   "metadata": {},
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    ">Mean Squared Error: the most commonly used metric that gives a higher penalty to large errors and vice versa,  [0,+∞)\n",
    " \n",
    ">MSE=1n∑i=1n(yi−ŷ i)2\n",
    " \n",
    "\n",
    "`sklearn.metrics.mean_squared_error`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b867920",
   "metadata": {},
   "source": [
    "# Mean Squared Logarithmic Error\n",
    "\n",
    ">Mean Squared Logarithmic Error: practically, this is the same as MSE, but we take the logarithm of the series. As a result, we give more >weight to small mistakes as well. This is usually used when the data has exponential trends,  [0,+∞)\n",
    " \n",
    "`MSLE=1n∑i=1n(log(1+yi)−log(1+ŷ i))2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905aa9d",
   "metadata": {},
   "source": [
    "# Mean Aboslute Percentage Error\n",
    "\n",
    ">Mean Absolute Percentage Error: this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain >the quality of the model to management,  [0,+∞)\n",
    " \n",
    "`MAPE=100n∑i=1n|yi−ŷ i|yi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a5f50",
   "metadata": {},
   "source": [
    "# Weigted Average\n",
    "\n",
    ">Weighted average is a simple modification to the moving average. The weights sum up to 1 with larger weights assigned to more recent observations.\n",
    "\n",
    "`ŷ t=∑n=1kωnyt+1−n`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d027c",
   "metadata": {},
   "source": [
    "# Double exponential smoothing\n",
    "\n",
    ">Up to now, the methods that we've discussed have been for a single future point prediction (with some nice smoothing). That is cool, but it is also not enough. Let's extend exponential smoothing so that we can predict two future points (of course, we will also include more smoothing).\n",
    "\n",
    ">Series decomposition will help us -- we obtain two components: intercept (i.e. level)  ℓ\n",
    "  >and slope (i.e. trend)  b\n",
    " >. We have learnt to predict intercept (or expected series value) with our previous methods; now, we will apply the same exponential smoothing to the trend by assuming that the >future direction of the time series changes depends on the previous weighted changes. As a result, we get the following set of functions:\n",
    "\n",
    "`ℓx=αyx+(1−α)(ℓx−1+bx−1)`\n",
    " \n",
    "\n",
    "`bx=β(ℓx−ℓx−1)+(1−β)bx−1`\n",
    " \n",
    "\n",
    "`ŷ x+1=ℓx+bx`\n",
    " \n",
    "\n",
    ">The first one describes the intercept, which, as before, depends on the current value of the series. The second term is now split into previous values of the level and of the trend. The second function describes the trend, which depends on the level changes at the current step and on the previous value of the trend. In this case, the  β\n",
    "  coefficient is a weight for exponential smoothing. The final prediction is the sum of the model values of the intercept and trend.\n",
    "\n",
    "\n",
    "\n",
    "## See below explanation and example:\n",
    "\n",
    "Perfect — let’s do this step by step, slowly, so the intuition sticks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Why Do We Need Double Smoothing?\n",
    "\n",
    "We just saw **why single exponential smoothing (SES) fails**:\n",
    "\n",
    "* It assumes data wiggles around a stable mean.\n",
    "* But when there’s a **trend**, it always lags — like chasing a moving car without knowing its speed.\n",
    "\n",
    "👉 So, to fix this, we don’t just want to know **“where the car is now”** (the *level*) — we also want to know **“how fast it’s moving”** (the *trend*).\n",
    "\n",
    "That’s the intuition behind **Double Exponential Smoothing (Holt’s Method)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 The Building Blocks\n",
    "\n",
    "Double smoothing splits the forecast into **two components**:\n",
    "\n",
    "1. **Level (ℓ)** = Where is the series *right now*?\n",
    "\n",
    "   * Like the car’s current position.\n",
    "   * Updated each time with smoothing (just like SES did).\n",
    "\n",
    "2. **Trend (b)** = How fast (and in which direction) is the series moving?\n",
    "\n",
    "   * Like the car’s speed.\n",
    "   * Also updated with smoothing, because trends can change over time.\n",
    "\n",
    "Then, the forecast is simply:\n",
    "\n",
    "$$\n",
    "\\text{Future} = \\text{Level} + \\text{Trend}\n",
    "$$\n",
    "\n",
    "👉 Position + Speed\n",
    "\n",
    "That’s the *big picture*.\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 The Formula (Don’t worry, we’ll unpack it gently)\n",
    "\n",
    "1. **Update Level (ℓ):**\n",
    "\n",
    "$$\n",
    "\\ell_t = \\alpha y_t + (1-\\alpha)(\\ell_{t-1} + b_{t-1})\n",
    "$$\n",
    "\n",
    "* Combines today’s actual value ($y_t$) and yesterday’s forecast (which already included yesterday’s level + trend).\n",
    "* **Goal:** Keep track of “where we are now,” but accounting for the fact that data is moving.\n",
    "\n",
    "2. **Update Trend (b):**\n",
    "\n",
    "$$\n",
    "b_t = \\beta (\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}\n",
    "$$\n",
    "\n",
    "* Looks at how the level has changed ($\\ell_t - \\ell_{t-1}$) = the new slope.\n",
    "* Mixes it with the old slope ($b_{t-1}$).\n",
    "* **Goal:** Keep track of “how fast the series is moving,” smoothed to avoid overreacting to noise.\n",
    "\n",
    "3. **Forecast:**\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = \\ell_t + b_t\n",
    "$$\n",
    "\n",
    "* Just like “position + speed = next position.”\n",
    "\n",
    "---\n",
    "\n",
    "Perfect — because your instinct is right: if we only use neat numbers like 10, 12, 14, 16, it looks too “clean” and the formula seems like magic. Let’s pick a slightly messy example where intuition becomes more obvious.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Series (not evenly spaced)\n",
    "\n",
    "Suppose we observe daily sales:\n",
    "\n",
    "$$\n",
    "10, \\; 13, \\; 15, \\; 18, \\; 17\n",
    "$$\n",
    "\n",
    "This looks like:\n",
    "\n",
    "* It’s generally trending **upward**,\n",
    "* But not in a perfect straight line (some noise).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Initialize\n",
    "\n",
    "We need a starting level and trend.\n",
    "\n",
    "* $ \\ell_0 = 10$ (first observation)\n",
    "* $ b_0 = 13 - 10 = 3$ (difference between first two points)\n",
    "\n",
    "Set smoothing parameters: α = 0.5, β = 0.5 (so we balance between old and new info).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Update step by step\n",
    "\n",
    "### At $t=2$ (actual = 13)\n",
    "\n",
    "* Level:\n",
    "\n",
    "$$\n",
    "\\ell_2 = 0.5(13) + 0.5(10 + 3) = 6.5 + 6.5 = 13\n",
    "$$\n",
    "\n",
    "* Trend:\n",
    "\n",
    "$$\n",
    "b_2 = 0.5(13 - 10) + 0.5(3) = 1.5 + 1.5 = 3\n",
    "$$\n",
    "\n",
    "### At $t=3$ (actual = 15)\n",
    "\n",
    "* Level:\n",
    "\n",
    "$$\n",
    "\\ell_3 = 0.5(15) + 0.5(13 + 3) = 7.5 + 8 = 15.5\n",
    "$$\n",
    "\n",
    "* Trend:\n",
    "\n",
    "$$\n",
    "b_3 = 0.5(15.5 - 13) + 0.5(3) = 1.25 + 1.5 = 2.75\n",
    "$$\n",
    "\n",
    "### At $t=4$ (actual = 18)\n",
    "\n",
    "* Level:\n",
    "\n",
    "$$\n",
    "\\ell_4 = 0.5(18) + 0.5(15.5 + 2.75) = 9 + 9.125 = 18.125\n",
    "$$\n",
    "\n",
    "* Trend:\n",
    "\n",
    "$$\n",
    "b_4 = 0.5(18.125 - 15.5) + 0.5(2.75) = 1.3125 + 1.375 = 2.6875\n",
    "$$\n",
    "\n",
    "### At $t=5$ (actual = 17)\n",
    "\n",
    "* Level:\n",
    "\n",
    "$$\n",
    "\\ell_5 = 0.5(17) + 0.5(18.125 + 2.6875) = 8.5 + 10.40625 = 18.90625\n",
    "$$\n",
    "\n",
    "* Trend:\n",
    "\n",
    "$$\n",
    "b_5 = 0.5(18.90625 - 18.125) + 0.5(2.6875) = 0.390625 + 1.34375 = 1.734375\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Forecast next point (t=6)\n",
    "\n",
    "$$\n",
    "\\hat{y}_6 = \\ell_5 + b_5 = 18.90625 + 1.734375 \\approx 20.64\n",
    "$$\n",
    "\n",
    "👉 Notice how this balances:\n",
    "\n",
    "* The *actual last value* was 17 (a dip),\n",
    "* But because the model knows there’s an *overall upward trend*, it doesn’t predict 17 again.\n",
    "* Instead, it predicts \\~20.6, respecting both the dip and the upward momentum.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Intuition\n",
    "\n",
    "* **Level** keeps track of “where the series seems to be right now.”\n",
    "* **Trend** adjusts for “the direction and speed it’s moving.”\n",
    "* Together, they prevent the model from overreacting to single dips/spikes while still capturing the overall slope.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **plot this messy example** side by side with **Single Smoothing** so you can *visually* see SES lagging while Double Smoothing projects forward?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83e19a",
   "metadata": {},
   "source": [
    "# Triple exponential smoothing a.k.a. Holt-Winters¶\n",
    "\n",
    "We've looked at exponential smoothing and double exponential smoothing. This time, we're going into triple exponential smoothing.\n",
    "\n",
    "As you could have guessed, the idea is to add a third component - seasonality. This means that we should not use this method if our time series is not expected to have seasonality. Seasonal components in the model will explain repeated variations around intercept and trend, and it will be specified by the length of the season, in other words by the period after which the variations repeat. For each observation in the season, there is a separate component; for example, if the length of the season is 7 days (a weekly seasonality), we will have 7 seasonal components, one for each day of the week.\n",
    "\n",
    "With this, let's write out a new system of equations:\n",
    "\n",
    "`ℓx=α(yx−sx−L)+(1−α)(ℓx−1+bx−1)`\n",
    "\n",
    "`bx=β(ℓx−ℓx−1)+(1−β)bx−1`\n",
    "\n",
    "`sx=γ(yx−ℓx)+(1−γ)sx−L`\n",
    "\n",
    "`ŷ x+m=ℓx+mbx+sx−L+1+(m−1)modL`\n",
    "\n",
    "The intercept now depends on the current value of the series minus any corresponding seasonal component. Trend remains unchanged, and the seasonal component depends on the current value of the series minus the intercept and on the previous value of the component. Take into account that the component is smoothed through all the available seasons; for example, if we have a Monday component, then it will only be averaged with other Mondays. You can read more on how averaging works and how the initial approximation of the trend and seasonal components is done here. Now that we have the seasonal component, we can predict not just one or two steps ahead but an arbitrary m\n",
    " future steps ahead, which is very encouraging.\n",
    "\n",
    "Below is the code for a triple exponential smoothing model, which is also known by the last names of its creators, Charles Holt and his student Peter Winters. Additionally, the Brutlag method was included in the model to produce confidence intervals:\n",
    "\n",
    "`ŷ maxx=ℓx−1+bx−1+sx−T+m⋅dt−T`\n",
    "\n",
    "`ŷ minx=ℓx−1+bx−1+sx−T−m⋅dt−T`\n",
    "\n",
    "`dt=γ∣yt−ŷ t∣+(1−γ)dt−T,`\n",
    "\n",
    "where T\n",
    " is the length of the season, d\n",
    " is the predicted deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95faab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
